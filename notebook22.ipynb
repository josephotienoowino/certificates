{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-05T10:37:35.939713Z","iopub.execute_input":"2022-11-05T10:37:35.940329Z","iopub.status.idle":"2022-11-05T10:37:35.951709Z","shell.execute_reply.started":"2022-11-05T10:37:35.940287Z","shell.execute_reply":"2022-11-05T10:37:35.950774Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"/kaggle/input/feedback-prize-english-language-learning/sample_submission.csv\n/kaggle/input/feedback-prize-english-language-learning/train.csv\n/kaggle/input/feedback-prize-english-language-learning/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout\nimport tensorflow.keras as keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom pandas import Series, DataFrame\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn as sn\n\ndf=pd.read_csv(\"../input/feedback-prize-english-language-learning/train.csv\")\nss=pd.read_csv(\"../input/feedback-prize-english-language-learning/sample_submission.csv\")\ndata_test=pd.read_csv(\"../input/feedback-prize-english-language-learning/test.csv\")\ndata_test= data_test['full_text']\ndf.columns.values,len(ss.columns.values)","metadata":{"execution":{"iopub.status.busy":"2022-11-05T10:37:35.954559Z","iopub.execute_input":"2022-11-05T10:37:35.955249Z","iopub.status.idle":"2022-11-05T10:37:36.054339Z","shell.execute_reply.started":"2022-11-05T10:37:35.955210Z","shell.execute_reply":"2022-11-05T10:37:36.053262Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(array(['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary',\n        'phraseology', 'grammar', 'conventions'], dtype=object),\n 7)"},"metadata":{}}]},{"cell_type":"code","source":"X_train=df['full_text']\ny_train=np.array(df.iloc[:,2:])\ntokenizer = Tokenizer(num_words=1000, split=' ') \ntokenizer.fit_on_texts(X_train.values)\nX_train = tokenizer.texts_to_sequences(X_train.values)\ndata_test= tokenizer.texts_to_sequences(data_test.values)\nX_train = pad_sequences(X_train)\ndata_test = pad_sequences(data_test)\nX_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-11-05T10:37:36.055940Z","iopub.execute_input":"2022-11-05T10:37:36.056443Z","iopub.status.idle":"2022-11-05T10:37:37.786788Z","shell.execute_reply.started":"2022-11-05T10:37:36.056400Z","shell.execute_reply":"2022-11-05T10:37:37.785687Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model=Sequential()\nmodel = keras.Sequential()\n# Add an Embedding layer expecting input vocab of size 5000, and\n# output embedding dimension of size 120.\nmodel.add(Embedding(input_dim=10000, output_dim=120))\n\n# Add a LSTM layer with 128 internal units.\nmodel.add(LSTM(128))\n# Add a Dropout layer with persent 80% out units.\nmodel.add(Dropout(0.8))\nfor i in range(7):\n    # Add a Dense layer with 128 units and activation relu.\n    model.add(Dense(128,activation='relu'))\n    # Add a Dropout layer with persent 70% out units.\n    model.add(Dropout(0.8))\n# Add a Dense layer with 6 units and activation relu.\nmodel.add(Dense(6,activation='relu'))\n#create compile with loss mean_sequard_error and optmizer adm\nmodel.compile(\n    loss = 'mse',\n    optimizer ='sgd'\n)","metadata":{"execution":{"iopub.status.busy":"2022-11-05T10:37:37.788583Z","iopub.execute_input":"2022-11-05T10:37:37.789031Z","iopub.status.idle":"2022-11-05T10:37:38.077486Z","shell.execute_reply.started":"2022-11-05T10:37:37.788995Z","shell.execute_reply":"2022-11-05T10:37:38.076570Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, y_train, epochs = 50)\nloss=model.evaluate(X_val,y_val)\nloss","metadata":{"execution":{"iopub.status.busy":"2022-11-05T10:37:38.080307Z","iopub.execute_input":"2022-11-05T10:37:38.080685Z","iopub.status.idle":"2022-11-05T10:43:02.609630Z","shell.execute_reply.started":"2022-11-05T10:37:38.080649Z","shell.execute_reply":"2022-11-05T10:43:02.608725Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Epoch 1/50\n110/110 [==============================] - 8s 54ms/step - loss: 9.8419\nEpoch 2/50\n110/110 [==============================] - 6s 54ms/step - loss: 6.9454\nEpoch 3/50\n110/110 [==============================] - 6s 53ms/step - loss: 2.6105\nEpoch 4/50\n110/110 [==============================] - 6s 53ms/step - loss: 1.1161\nEpoch 5/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.9162\nEpoch 6/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.8004\nEpoch 7/50\n110/110 [==============================] - 6s 54ms/step - loss: 0.7491\nEpoch 8/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.7137\nEpoch 9/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.6745\nEpoch 10/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.6500\nEpoch 11/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.6280\nEpoch 12/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.6088\nEpoch 13/50\n110/110 [==============================] - 6s 54ms/step - loss: 0.6135\nEpoch 14/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.6051\nEpoch 15/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.5783\nEpoch 16/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.5760\nEpoch 17/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.5581\nEpoch 18/50\n110/110 [==============================] - 6s 54ms/step - loss: 0.5608\nEpoch 19/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.5491\nEpoch 20/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.5408\nEpoch 21/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.5428\nEpoch 22/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.5306\nEpoch 23/50\n110/110 [==============================] - 6s 54ms/step - loss: 0.5368\nEpoch 24/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.5364\nEpoch 25/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.5306\nEpoch 26/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.5220\nEpoch 27/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.5168\nEpoch 28/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.5053\nEpoch 29/50\n110/110 [==============================] - 6s 54ms/step - loss: 0.5132\nEpoch 30/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.4985\nEpoch 31/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.4893\nEpoch 32/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.4950\nEpoch 33/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.4938\nEpoch 34/50\n110/110 [==============================] - 6s 54ms/step - loss: 0.4930\nEpoch 35/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.4863\nEpoch 36/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.4855\nEpoch 37/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.4840\nEpoch 38/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.4838\nEpoch 39/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.4801\nEpoch 40/50\n110/110 [==============================] - 6s 54ms/step - loss: 0.4858\nEpoch 41/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.4738\nEpoch 42/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.4776\nEpoch 43/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.4775\nEpoch 44/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.4725\nEpoch 45/50\n110/110 [==============================] - 6s 54ms/step - loss: 0.4698\nEpoch 46/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.4653\nEpoch 47/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.4666\nEpoch 48/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.4654\nEpoch 49/50\n110/110 [==============================] - 6s 53ms/step - loss: 0.4659\nEpoch 50/50\n110/110 [==============================] - 6s 54ms/step - loss: 0.4593\n13/13 [==============================] - 1s 23ms/step - loss: 0.4279\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"0.42788341641426086"},"metadata":{}}]},{"cell_type":"code","source":"y_predict=model.predict(data_test)\nss.iloc[:,1:]=y_predict\nss.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-11-05T10:43:02.610996Z","iopub.execute_input":"2022-11-05T10:43:02.611392Z","iopub.status.idle":"2022-11-05T10:43:02.962323Z","shell.execute_reply.started":"2022-11-05T10:43:02.611355Z","shell.execute_reply":"2022-11-05T10:43:02.961182Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"ss","metadata":{"execution":{"iopub.status.busy":"2022-11-05T10:43:02.963692Z","iopub.execute_input":"2022-11-05T10:43:02.964055Z","iopub.status.idle":"2022-11-05T10:43:02.976230Z","shell.execute_reply.started":"2022-11-05T10:43:02.964019Z","shell.execute_reply":"2022-11-05T10:43:02.975060Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n0  0000C359D63E  3.093692  2.995645    3.200467     3.084649  2.995426   \n1  000BAD50D026  3.093692  2.995645    3.200467     3.084649  2.995426   \n2  00367BB2546B  3.093692  2.995645    3.200467     3.084649  2.995426   \n\n   conventions  \n0     3.043636  \n1     3.043636  \n2     3.043636  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000C359D63E</td>\n      <td>3.093692</td>\n      <td>2.995645</td>\n      <td>3.200467</td>\n      <td>3.084649</td>\n      <td>2.995426</td>\n      <td>3.043636</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000BAD50D026</td>\n      <td>3.093692</td>\n      <td>2.995645</td>\n      <td>3.200467</td>\n      <td>3.084649</td>\n      <td>2.995426</td>\n      <td>3.043636</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00367BB2546B</td>\n      <td>3.093692</td>\n      <td>2.995645</td>\n      <td>3.200467</td>\n      <td>3.084649</td>\n      <td>2.995426</td>\n      <td>3.043636</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}